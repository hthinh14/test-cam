import streamlit as st
import cv2
import mediapipe as mp
import numpy as np
import pickle
from collections import deque
import json
import av # Th∆∞ vi·ªán c·∫ßn thi·∫øt cho WebRTC/Streamlit
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, WebRtcMode 
import joblib

# ==============================
# C·∫§U H√åNH C∆† B·∫¢N
# ==============================
# ƒê√£ ƒë·ªïi t√™n file model v√† scaler
MODEL_PATH = "softmax_model_best.pkl"
SCALER_PATH = "scaler.pkl"
LABEL_MAP_PATH = "label_map.json"

SMOOTH_WINDOW = 8
EPS = 1e-6
WINDOW_SIZE = 30 # C·ª≠a s·ªï khung h√¨nh ƒë·ªÉ t√≠nh ƒë·∫∑c tr∆∞ng th·ªëng k√™

def predict(X, model):
    """S·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c d·ª± ƒëo√°n chu·∫©n c·ªßa m√¥ h√¨nh Softmax/Logistic ƒëa l·ªõp."""
    # Gi·∫£ ƒë·ªãnh m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán (v√≠ d·ª•: Sklearn) c√≥ h√†m predict
    return model.predict(X) 

@st.cache_resource
def load_assets():
    """T·∫£i model, scaler v√† label map"""
    try:
        # Load model (gi·ªØ nguy√™n)
        with open(MODEL_PATH, "rb") as f:
            model = joblib.load(f)
            
        # Load scaler 
        with open(SCALER_PATH, "rb") as f:
            scaler_data = joblib.load(f) # T·∫£i ƒë·ªëi t∆∞·ª£ng

        # --- PH·∫¶N KH·∫ÆC PH·ª§C L·ªñI SCALER KEY ---
        if isinstance(scaler_data, dict):
            # Ki·ªÉm tra c√°c key ph·ªï bi·∫øn n·∫øu key 'mean' v√† 'std' kh√¥ng t·ªìn t·∫°i
            if 'mean' in scaler_data and 'std' in scaler_data:
                 mean_data = scaler_data['mean']
                 std_data = scaler_data['std']
            elif 'avg' in scaler_data and 'stdev' in scaler_data:
                 # Th·ª≠ key thay th·∫ø
                 mean_data = scaler_data['avg']
                 std_data = scaler_data['stdev']
            else:
                 raise ValueError("File scaler l√† dictionary nh∆∞ng thi·∫øu key 'mean'/'std' ho·∫∑c 'avg'/'stdev'.")
        else:
             # N·∫øu kh√¥ng ph·∫£i dict, kh√¥ng ph·∫£i Sklearn object, l·ªói l√† kh√¥ng th·ªÉ load
             raise TypeError("File scaler kh√¥ng ph·∫£i l√† dictionary ch·ª©a mean/std.")
            
        # ... (T·∫£i label map v√† tr·∫£ v·ªÅ)
        with open(LABEL_MAP_PATH, "r") as f:
            label_map = json.load(f)
        id2label = {v: k for k, v in label_map.items()}
        
        return model, mean_data, std_data, id2label # Tr·∫£ v·ªÅ mean v√† std ƒë√£ tr√≠ch xu·∫•t

    # ... (X·ª≠ l√Ω c√°c ngo·∫°i l·ªá FileNotFoundError, v.v.)
    except (ValueError, TypeError) as e:
        st.error(f"L·ªói: Ki·ªÉm tra c·∫•u tr√∫c file scale.pkl. {e}")
        st.stop()
    except Exception as e:
        st.error(f"L·ªói Load: {e}")
        st.stop()
# T·∫£i t√†i s·∫£n
model, mean, std, id2label = load_assets() # ƒê√£ ƒë·ªïi t√™n bi·∫øn t·ª´ tree th√†nh model
classes = list(id2label.values())


# ==============================
# 3Ô∏è‚É£ H√ÄM T√çNH ƒê·∫∂C TR∆ØNG (Gi·ªØ nguy√™n)
# ==============================
mp_face_mesh = mp.solutions.face_mesh
EYE_LEFT_IDX = np.array([33, 159, 145, 133, 153, 144])
EYE_RIGHT_IDX = np.array([362, 386, 374, 263, 380, 385])
MOUTH_IDX = np.array([61, 291, 0, 17, 78, 308])

def eye_aspect_ratio(landmarks, left=True):
    idx = EYE_LEFT_IDX if left else EYE_RIGHT_IDX
    pts = landmarks[idx, :2]
    A = np.linalg.norm(pts[1] - pts[5])
    B = np.linalg.norm(pts[2] - pts[4])
    C = np.linalg.norm(pts[0] - pts[3])
    return (A + B) / (2.0 * (C + EPS))

def mouth_aspect_ratio(landmarks):
    pts = landmarks[MOUTH_IDX, :2]
    A = np.linalg.norm(pts[0] - pts[1])
    B = np.linalg.norm(pts[4] - pts[5])
    C = np.linalg.norm(pts[2] - pts[3])
    return (A + B) / (2.0 * (C + EPS))

def head_pose_yaw_pitch_roll(landmarks):
    left_eye = landmarks[33][:2]
    right_eye = landmarks[263][:2]
    nose = landmarks[1][:2]
    chin = landmarks[152][:2]
    dx = right_eye[0] - left_eye[0]
    dy = right_eye[1] - left_eye[1]
    roll = np.degrees(np.arctan2(dy, dx + EPS))
    interocular = np.linalg.norm(right_eye - left_eye) + EPS
    eyes_center = (left_eye + right_eye) / 2.0
    yaw = np.degrees(np.arctan2((nose[0] - eyes_center[0]), interocular))
    baseline = chin - eyes_center
    pitch = np.degrees(np.arctan2((nose[1] - eyes_center[1]), (np.linalg.norm(baseline) + EPS)))
    return yaw, pitch, roll

def get_extra_features(landmarks):
    nose, chin = landmarks[1], landmarks[152]
    angle_pitch_extra = np.degrees(np.arctan2(chin[1] - nose[1], (chin[2] - nose[2]) + EPS))
    forehead_y = np.mean(landmarks[[10, 338, 297, 332, 284], 1])
    cheek_dist = np.linalg.norm(landmarks[50] - landmarks[280])
    return angle_pitch_extra, forehead_y, cheek_dist


# ==============================
# 4Ô∏è‚É£ WEBRTC VIDEO PROCESSOR (Logic x·ª≠ l√Ω Real-time)
# ==============================
class DrowsinessProcessor(VideoProcessorBase):
    def __init__(self):
        # S·ª≠ d·ª•ng model ƒë√£ t·∫£i global
        self.model = model 
        self.mean = mean
        self.std = std
        self.id2label = id2label
        self.face_mesh = mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=True)
        self.frame_queue = deque(maxlen=WINDOW_SIZE)
        self.pred_queue = deque(maxlen=SMOOTH_WINDOW)
        self.last_pred_label = "CH·ªú D·ªÆ LI·ªÜU" 

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        frame_array = frame.to_ndarray(format="bgr24")
        h, w = frame_array.shape[:2]
        rgb = cv2.cvtColor(frame_array, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb)
        
        # --- 1. TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG ---
        if results.multi_face_landmarks:
            landmarks = np.array([[p.x * w, p.y * h, p.z * w] for p in results.multi_face_landmarks[0].landmark])

            # T√≠nh 9 ƒë·∫∑c tr∆∞ng c∆° b·∫£n
            ear_l = eye_aspect_ratio(landmarks, True); ear_r = eye_aspect_ratio(landmarks, False); mar = mouth_aspect_ratio(landmarks)
            yaw, pitch, roll = head_pose_yaw_pitch_roll(landmarks)
            angle_pitch_extra, forehead_y, cheek_dist = get_extra_features(landmarks)

            feat = np.array([ear_l, ear_r, mar, yaw, pitch, roll, 
                             angle_pitch_extra, forehead_y, cheek_dist], dtype=np.float32)
            self.frame_queue.append(feat) 
            
            # --- 2. D·ª∞ ƒêO√ÅN KHI ƒê·ª¶ 30 KHUNG H√åNH ---
            if len(self.frame_queue) == WINDOW_SIZE:
                window = np.array(self.frame_queue)
                
                # T√≠nh 24 ƒë·∫∑c tr∆∞ng
                mean_feats = window.mean(axis=0); std_feats = window.std(axis=0)
                yaw_diff = np.mean(np.abs(np.diff(window[:, 3]))); pitch_diff = np.mean(np.abs(np.diff(window[:, 4]))); roll_diff = np.mean(np.abs(np.diff(window[:, 5])))
                mar_mean = np.mean(window[:, 2]); ear_mean = np.mean((window[:, 0] + window[:, 1]) / 2.0)
                mar_ear_ratio = mar_mean / (ear_mean + EPS); yaw_pitch_ratio = np.mean(np.abs(window[:, 3])) / (np.mean(np.abs(window[:, 4])) + EPS)
                feats_24 = np.concatenate([mean_feats, std_feats, [yaw_diff, pitch_diff, roll_diff, np.max(window[:, 2]), mar_ear_ratio, yaw_pitch_ratio]])

                # Chu·∫©n h√≥a, D·ª± ƒëo√°n
                feats_scaled = (feats_24 - self.mean) / self.std
                
                # S·ª¨ D·ª§NG PH∆Ø∆†NG TH·ª®C D·ª∞ ƒêO√ÅN C·ª¶A SKLEARN/SOFTMAX
                # (ƒê·∫£m b·∫£o input c√≥ shape (1, 24))
                pred_idx = self.model.predict(np.expand_dims(feats_scaled, axis=0))[0] 
                
                pred_label = self.id2label.get(pred_idx, f"Class {pred_idx}")
                self.pred_queue.append(pred_label)

                # X√≥a 15 khung h√¨nh c≈© (overlap)
                for _ in range(15): 
                    if self.frame_queue:
                        self.frame_queue.popleft() 
        
        # --- 3. SMOOTHING ---
        if len(self.pred_queue) > 0:
            self.last_pred_label = max(set(self.pred_queue), key=self.pred_queue.count)

        # --- 4. HI·ªÇN TH·ªä K·∫æT QU·∫¢ ---
        cv2.putText(frame_array, f"Trang thai: {self.last_pred_label.upper()}", (10, 70), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 3)

        return av.VideoFrame.from_ndarray(frame_array, format="bgr24")

# ==============================
# 1Ô∏è‚É£ GIAO DI·ªÜN STREAMLIT CH√çNH
# ==============================
st.set_page_config(page_title="Demo Softmax", layout="wide")
st.title("üß† Nh·∫≠n di·ªán tr·∫°ng th√°i m·∫•t t·∫≠p trung b·∫±ng m√¥ h√¨nh h·ªçc m√°y.")
st.success(f"M√¥ h√¨nh s·∫µn s√†ng! C√°c nh√£n: {classes}")
st.warning("Vui l√≤ng ch·∫•p nh·∫≠n y√™u c·∫ßu truy c·∫≠p camera t·ª´ tr√¨nh duy·ªát c·ªßa b·∫°n.")
st.markdown("---")


# Kh·ªüi t·∫°o WebRTC Streamer
webrtc_streamer(
    key="softmax_driver_live",
    mode=WebRtcMode.SENDRECV,
    # C·∫•u h√¨nh STUN servers ƒë·ªÉ thi·∫øt l·∫≠p k·∫øt n·ªëi
    rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
    video_processor_factory=DrowsinessProcessor,
    media_stream_constraints={"video": True, "audio": False}, # Ch·ªâ b·∫≠t video
    async_processing=True, # Cho ph√©p x·ª≠ l√Ω kh√¥ng ƒë·ªìng b·ªô (tƒÉng t·ªëc ƒë·ªô)
)

WINDOW_SIZE = 30 # C·ª≠a s·ªï khung h√¨nh ƒë·ªÉ t√≠nh ƒë·∫∑c tr∆∞ng th·ªëng k√™

# ==============================
# H√ÄM D·ª∞ ƒêO√ÅN V√Ä LOAD ASSETS
# ==============================
def predict_one(x, tree):
    if not isinstance(tree, dict):
        return tree
    if x[tree["feature"]] <= tree["threshold"]:
        return predict_one(x, tree["left"])
    else:
        return predict_one(x, tree["right"])

def predict(X, tree):
    return np.array([predict_one(x, tree) for x in X])

@st.cache_resource
def load_assets():
    """T·∫£i model, scaler v√† label map"""
    try:
        # 1. T·∫£i m√¥ h√¨nh Softmax b·∫±ng joblib.load()
        with open(MODEL_PATH, "rb") as f:
            # S·ª¨A L·ªñI: S·ª≠ d·ª•ng joblib.load thay v√¨ pickle.load
            # Gi·∫£ ƒë·ªãnh joblib l√† th∆∞ vi·ªán d√πng ƒë·ªÉ l∆∞u model Softmax
            model = joblib.load(f) 
            
        # 2. T·∫£i scaler (N·∫øu scaler c≈©ng ƒë∆∞·ª£c l∆∞u b·∫±ng joblib)
        with open(SCALER_PATH, "rb") as f:
            # S·ª¨A L·ªñI: S·ª≠ d·ª•ng joblib.load thay v√¨ pickle.load
            scaler_data = joblib.load(f)
            
        with open(LABEL_MAP_PATH, "r") as f:
            label_map = json.load(f)
        id2label = {v: k for k, v in label_map.items()}
        
        return model, scaler_data["mean"], scaler_data["std"], id2label
    except FileNotFoundError as e:
        st.error(f"L·ªói File: Kh√¥ng t√¨m th·∫•y file t√†i nguy√™n. Vui l√≤ng ki·ªÉm tra ƒë∆∞·ªùng d·∫´n: {e.filename}")
        st.stop()
    except Exception as e:
        # N·∫øu l·ªói v·∫´n x·∫£y ra, √≠t nh·∫•t n√≥ kh√¥ng c√≤n l√† l·ªói load key
        st.error(f"L·ªói Load (Joblib): Ki·ªÉm tra c·∫•u tr√∫c file model: {e}")
        st.stop()

# T·∫£i t√†i s·∫£n (Ch·∫°y m·ªôt l·∫ßn khi kh·ªüi ƒë·ªông ·ª©ng d·ª•ng)
tree, mean, std, id2label = load_assets()
classes = list(id2label.values())


# ==============================
# 3Ô∏è‚É£ H√ÄM T√çNH ƒê·∫∂C TR∆ØNG (Gi·ªØ nguy√™n)
# ==============================
mp_face_mesh = mp.solutions.face_mesh
EYE_LEFT_IDX = np.array([33, 159, 145, 133, 153, 144])
EYE_RIGHT_IDX = np.array([362, 386, 374, 263, 380, 385])
MOUTH_IDX = np.array([61, 291, 0, 17, 78, 308])

def eye_aspect_ratio(landmarks, left=True):
    idx = EYE_LEFT_IDX if left else EYE_RIGHT_IDX
    pts = landmarks[idx, :2]
    A = np.linalg.norm(pts[1] - pts[5])
    B = np.linalg.norm(pts[2] - pts[4])
    C = np.linalg.norm(pts[0] - pts[3])
    return (A + B) / (2.0 * (C + EPS))

def mouth_aspect_ratio(landmarks):
    pts = landmarks[MOUTH_IDX, :2]
    A = np.linalg.norm(pts[0] - pts[1])
    B = np.linalg.norm(pts[4] - pts[5])
    C = np.linalg.norm(pts[2] - pts[3])
    return (A + B) / (2.0 * (C + EPS))

def head_pose_yaw_pitch_roll(landmarks):
    left_eye = landmarks[33][:2]
    right_eye = landmarks[263][:2]
    nose = landmarks[1][:2]
    chin = landmarks[152][:2]
    dx = right_eye[0] - left_eye[0]
    dy = right_eye[1] - left_eye[1]
    roll = np.degrees(np.arctan2(dy, dx + EPS))
    interocular = np.linalg.norm(right_eye - left_eye) + EPS
    eyes_center = (left_eye + right_eye) / 2.0
    yaw = np.degrees(np.arctan2((nose[0] - eyes_center[0]), interocular))
    baseline = chin - eyes_center
    pitch = np.degrees(np.arctan2((nose[1] - eyes_center[1]), (np.linalg.norm(baseline) + EPS)))
    return yaw, pitch, roll

def get_extra_features(landmarks):
    nose, chin = landmarks[1], landmarks[152]
    angle_pitch_extra = np.degrees(np.arctan2(chin[1] - nose[1], (chin[2] - nose[2]) + EPS))
    forehead_y = np.mean(landmarks[[10, 338, 297, 332, 284], 1])
    cheek_dist = np.linalg.norm(landmarks[50] - landmarks[280])
    return angle_pitch_extra, forehead_y, cheek_dist


# ==============================
# 4Ô∏è‚É£ WEBRTC VIDEO PROCESSOR (Logic x·ª≠ l√Ω Real-time)
# ==============================
class DrowsinessProcessor(VideoProcessorBase):
    def __init__(self):
        # T·∫£i t√†i s·∫£n (s·ª≠ d·ª•ng global assets ƒë√£ t·∫£i qua st.cache_resource)
        self.tree = tree
        self.mean = mean
        self.std = std
        self.id2label = id2label
        self.face_mesh = mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=True)
        self.frame_queue = deque(maxlen=WINDOW_SIZE)
        self.pred_queue = deque(maxlen=SMOOTH_WINDOW)
        self.last_pred_label = "CH·ªú D·ªÆ LI·ªÜU" # Tr·∫°ng th√°i ban ƒë·∫ßu

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        # Chuy·ªÉn khung h√¨nh WebRTC (AV) sang NumPy array (BGR)
        frame_array = frame.to_ndarray(format="bgr24")
        h, w = frame_array.shape[:2]
        rgb = cv2.cvtColor(frame_array, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb)
        
        # --- 1. TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG ---
        if results.multi_face_landmarks:
            landmarks = np.array([[p.x * w, p.y * h, p.z * w] for p in results.multi_face_landmarks[0].landmark])

            # T√≠nh 9 ƒë·∫∑c tr∆∞ng c∆° b·∫£n
            ear_l = eye_aspect_ratio(landmarks, True); ear_r = eye_aspect_ratio(landmarks, False); mar = mouth_aspect_ratio(landmarks)
            yaw, pitch, roll = head_pose_yaw_pitch_roll(landmarks)
            angle_pitch_extra, forehead_y, cheek_dist = get_extra_features(landmarks)

            feat = np.array([ear_l, ear_r, mar, yaw, pitch, roll, 
                             angle_pitch_extra, forehead_y, cheek_dist], dtype=np.float32)
            self.frame_queue.append(feat) 
            
            # --- 2. D·ª∞ ƒêO√ÅN KHI ƒê·ª¶ 30 KHUNG H√åNH ---
            if len(self.frame_queue) == WINDOW_SIZE:
                window = np.array(self.frame_queue)
                
                # T√≠nh 24 ƒë·∫∑c tr∆∞ng
                mean_feats = window.mean(axis=0); std_feats = window.std(axis=0)
                yaw_diff = np.mean(np.abs(np.diff(window[:, 3]))); pitch_diff = np.mean(np.abs(np.diff(window[:, 4]))); roll_diff = np.mean(np.abs(np.diff(window[:, 5])))
                mar_mean = np.mean(window[:, 2]); ear_mean = np.mean((window[:, 0] + window[:, 1]) / 2.0)
                mar_ear_ratio = mar_mean / (ear_mean + EPS); yaw_pitch_ratio = np.mean(np.abs(window[:, 3])) / (np.mean(np.abs(window[:, 4])) + EPS)
                feats_24 = np.concatenate([mean_feats, std_feats, [yaw_diff, pitch_diff, roll_diff, np.max(window[:, 2]), mar_ear_ratio, yaw_pitch_ratio]])

                # Chu·∫©n h√≥a, D·ª± ƒëo√°n
                feats_scaled = (feats_24 - self.mean) / self.std
                pred_idx = predict(np.expand_dims(feats_scaled, axis=0), self.tree)[0] 
                pred_label = self.id2label.get(pred_idx, f"Class {pred_idx}")
                self.pred_queue.append(pred_label)

                # X√≥a 15 khung h√¨nh c≈© (overlap)
                for _ in range(15): 
                    if self.frame_queue:
                        self.frame_queue.popleft() 
        
        # --- 3. SMOOTHING ---
        if len(self.pred_queue) > 0:
            self.last_pred_label = max(set(self.pred_queue), key=self.pred_queue.count)

        # --- 4. HI·ªÇN TH·ªä K·∫æT QU·∫¢ ---
        cv2.putText(frame_array, f"Trang thai: {self.last_pred_label.upper()}", (10, 70), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 3)

        return av.VideoFrame.from_ndarray(frame_array, format="bgr24")

# ==============================
# 1Ô∏è‚É£ GIAO DI·ªÜN STREAMLIT CH√çNH
# ==============================
st.set_page_config(page_title="Demo Bu·ªìn Ng·ªß", layout="wide")
st.title("üò¥ Nh·∫≠n di·ªán tr·∫°ng th√°i m·∫•t t·∫≠p trung b·∫±ng m√¥ h√¨nh h·ªçc m√°y.")
st.success(f"M√¥ h√¨nh s·∫µn s√†ng! C√°c nh√£n: {classes}")
st.warning("Vui l√≤ng ch·∫•p nh·∫≠n y√™u c·∫ßu truy c·∫≠p camera t·ª´ tr√¨nh duy·ªát c·ªßa b·∫°n.")
st.markdown("---")


# Kh·ªüi t·∫°o WebRTC Streamer
webrtc_streamer(
    key="decision_tree_driver_live",
    mode=WebRtcMode.SENDRECV,
    # C·∫•u h√¨nh STUN servers ƒë·ªÉ thi·∫øt l·∫≠p k·∫øt n·ªëi
    rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
    video_processor_factory=DrowsinessProcessor,
    media_stream_constraints={"video": True, "audio": False}, # Ch·ªâ b·∫≠t video
    async_processing=True, # Cho ph√©p x·ª≠ l√Ω kh√¥ng ƒë·ªìng b·ªô (tƒÉng t·ªëc ƒë·ªô)
)